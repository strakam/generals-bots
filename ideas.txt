JAX Implementation Improvement Ideas
=====================================
Generated: 2025-12-21

## Performance Improvements

### 3. Vectorize Action Execution
Currently executes actions sequentially (player 0, then player 1). Could use `vmap` to execute 
both simultaneously (if they don't interfere with each other).
Location: generals/core/game_jax.py:359-360

### 4. Remove Nested lax.cond Chains
Lines 241-280 in `_apply_move` have many nested `lax.cond` - these could be refactored to 
reduce branching overhead.
Location: generals/core/game_jax.py:202-303

### 5. compute_valid_move_mask Loop Unrolling
Lines 79-102: The `for dir_idx in range(4)` could be unrolled manually or use `lax.scan` 
for better JIT compilation.
Location: generals/core/action_jax.py:79-102

### 6. Debug Print Left in Code ⚠️ BUG
Lines 217-218 in `jax_vectorized_env.py` - there's a debug print and early return!
Location: examples/jax_vectorized_env.py:217-218

## Gym Environment Usability

### 7. Missing Gym API Methods ✅ COMPLETED
Added standard Gym interface:
- ✅ `metadata` property (render_modes, render_fps)
- ✅ `action_space` property (placeholder for now)
- ✅ `observation_space` property (placeholder for now)
- ✅ `render()` method (placeholder implementation)
- ✅ `close()` method
- ✅ `seed()` method
Location: examples/jax_vectorized_env.py - VectorizedJaxEnv class

### 8. Inconsistent Return Types ✅ COMPLETED
Fixed to use consistent JAX types:
- ✅ `reset()` now returns (ObservationJax, GameInfo) - both NamedTuples
- ✅ `step()` returns (ObservationJax, rewards, terminated, truncated, GameInfo)
- ✅ All return values are JAX arrays or NamedTuples (no mixed numpy/dict)
- ✅ Updated to Gym v0.26+ API (5-tuple return with terminated/truncated)
Location: examples/jax_vectorized_env.py:77-111

### 9. Auto-Reset on Done ✅ COMPLETED
Implemented automatic environment reset when episodes finish:
- ✅ JIT-compiled auto-reset function (zero Python overhead)
- ✅ Uses `jnp.where` to conditionally replace terminated states
- ✅ Pre-batched initial state cached for fast reset
- ✅ Observations returned are from AFTER reset
- ✅ Rewards/infos returned are from BEFORE reset (correct semantics)
- ✅ Performance maintained: ~45k steps/sec (no slowdown!)
- ✅ Verified correctness: terminated envs have time=1, winner=-1 after reset
Location: examples/jax_vectorized_env.py - step() method

### 10. Single Player vs Multi-Player Confusion
Gym envs are typically single-agent. Returning observations for BOTH players `[num_envs, 2, ...]`.
Should either:
- Make it single-player only
- Clearly separate into PettingZoo-style multi-agent API
Location: examples/jax_vectorized_env.py - overall design

### 11. Reward Function Too Simple
Line 96-98: Reward is just land difference. Should be configurable. Also returns rewards for 
BOTH players when Gym expects single agent.
Location: examples/jax_vectorized_env.py:96-98

### 12. No Episode Truncation
Missing `truncated` return (Gym v0.26+). Should return 5-tuple: 
`(obs, reward, terminated, truncated, info)`
Location: examples/jax_vectorized_env.py:77

### 13. Grid Generation in Reset
Lines 60-62: Uses slow `np.vectorize(ord)` in hot path. Should pre-compute grids or use pure JAX.
Location: examples/jax_vectorized_env.py:60-62

### 14. Different Grids Per Environment
All envs use the same grid (line 59 comment). For diverse training, should support different 
grids per env.
Location: examples/jax_vectorized_env.py:59

### 15. Action Masking Not Integrated
Have `compute_valid_move_mask` but it's not returned in the `info` dict. Standard practice is 
to include `"action_mask"` in info for masked action sampling.
Location: examples/jax_vectorized_env.py:77-111

### 16. No Metadata
Missing `metadata` dict with render modes, etc.
Location: examples/jax_vectorized_env.py - VectorizedJaxEnv class

## Priority Order (Quick Wins by Impact)

1. **Remove debug code** (lines 217-218) - CRITICAL BUG
2. **Convert state to NamedTuple** - BIG performance gain
3. **Add action masks to info dict** - Usability
4. **Fix reward/done handling** for single-agent or multi-agent clarity
5. **Optimize get_visibility** with convolution - Performance
6. **Unroll compute_valid_move_mask loop** - Performance
7. **Add Gym API methods** - Usability
8. **Auto-reset on done** - Usability
9. **Configurable rewards** - Flexibility
10. **Episode truncation** - Gym compatibility
